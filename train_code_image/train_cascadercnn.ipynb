{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Install mmdetection\n",
    "!rm -rf mmdetection\n",
    "!git clone https://github.com/open-mmlab/mmdetection.git\n",
    "%cd mmdetection\n",
    "\n",
    "!pip install -e ."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "# Change the wnd username and project name below\n",
    "wnb_username = ''\n",
    "wnb_project_name = ''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check mmcv installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())\n",
    "\n",
    "# Check MMDetection installation\n",
    "from mmdet.apis import set_random_seed\n",
    "\n",
    "# Imports\n",
    "import mmdet\n",
    "from mmdet.apis import set_random_seed\n",
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.apis import train_detector\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "global_seed = 111\n",
    "\n",
    "def set_seed(seed=global_seed):\n",
    "    \"\"\"Sets the random seeds.\"\"\"\n",
    "    set_random_seed(seed, deterministic=False)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from mmcv import Config\n",
    "\n",
    "baseline_cfg_path = \"./configs/cascade_rcnn/cascade_rcnn_x101_32x4d_fpn_1x_coco.py\"\n",
    "cfg = Config.fromfile(baseline_cfg_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_name = 'cascade_rcnn_x101_32x4d_fpn_1x'\n",
    "fold = 0\n",
    "job = 0\n",
    "\n",
    "# Folder to store model logs and weight files\n",
    "job_folder = f'../job{job}_{model_name}_fold{fold}'\n",
    "cfg.work_dir = job_folder\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = global_seed\n",
    "\n",
    "if not os.path.exists(job_folder):\n",
    "    os.makedirs(job_folder)\n",
    "\n",
    "print(\"Job folder:\", job_folder)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set the number of classes\n",
    "for head in cfg.model.roi_head.bbox_head:\n",
    "    head.num_classes = 1\n",
    "\n",
    "cfg.gpu_ids = [0]\n",
    "\n",
    "# Setting pretrained model in the init_cfg which is required \n",
    "# for transfer learning as per the latest MMdetection update\n",
    "cfg.model.backbone.init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')\n",
    "cfg.model.backbone.init_cfg=dict(type='Pretrained', checkpoint='open-mmlab://resnext101_32x4d')\n",
    "cfg.model.pop('pretrained', None)\n",
    "\n",
    "cfg.runner.max_epochs = 12 # Epochs for the runner that runs the workflow \n",
    "cfg.total_epochs = 12\n",
    "\n",
    "# Learning rate of optimizers. The LR is divided by 8 since the config file is originally for 8 GPUs\n",
    "cfg.optimizer.lr = 0.02/8\n",
    "\n",
    "## Learning rate scheduler config used to register LrUpdater hook\n",
    "cfg.lr_config = dict(\n",
    "    policy='CosineAnnealing', # The policy of scheduler, also support CosineAnnealing, Cyclic, etc. Refer to details of supported LrUpdater from https://github.com/open-mmlab/mmcv/blob/master/mmcv/runner/hooks/lr_updater.py#L9.\n",
    "    by_epoch=False,\n",
    "    warmup='linear', # The warmup policy, also support `exp` and `constant`.\n",
    "    warmup_iters=500, # The number of iterations for warmup\n",
    "    warmup_ratio=0.001, # The ratio of the starting learning rate used for warmup\n",
    "    min_lr=1e-07)\n",
    "\n",
    "# config to register logger hook\n",
    "cfg.log_config.interval = 20 # Interval to print the log\n",
    "\n",
    "# Config to set the checkpoint hook, Refer to https://github.com/open-mmlab/mmcv/blob/master/mmcv/runner/hooks/checkpoint.py for implementation.\n",
    "cfg.checkpoint_config.interval = 1 # The save interval is 1\n",
    "\n",
    "cfg.dataset_type = 'CocoDataset' # Dataset type, this will be used to define the dataset\n",
    "cfg.classes = (\"Covid_Abnormality\",)\n",
    "\n",
    "cfg.data.train.img_prefix = '../data-512/train' # Prefix of image path\n",
    "cfg.data.train.classes = cfg.classes\n",
    "cfg.data.train.ann_file = f'../data_path/train_annotations_fold{fold}.json'\n",
    "cfg.data.train.type='CocoDataset'\n",
    "\n",
    "cfg.data.val.img_prefix = '../data-512/train' # Prefix of image path\n",
    "cfg.data.val.classes = cfg.classes\n",
    "cfg.data.val.ann_file = f'../data_path/val_annotations_fold{fold}.json'\n",
    "cfg.data.val.type='CocoDataset'\n",
    "\n",
    "cfg.data.test.img_prefix = '../data-512/train' # Prefix of image path\n",
    "cfg.data.test.classes = cfg.classes\n",
    "cfg.data.test.ann_file =  f'../data_path/val_annotations_fold{fold}.json'\n",
    "cfg.data.test.type='CocoDataset'\n",
    "\n",
    "cfg.data.samples_per_gpu = 4 # Batch size of a single GPU used in testing\n",
    "cfg.data.workers_per_gpu = 2 # Worker to pre-fetch data for each single GPU\n",
    "\n",
    "# The config to build the evaluation hook, refer to https://github.com/open-mmlab/mmdetection/blob/master/mmdet/core/evaluation/eval_hooks.py#L7 for more details.\n",
    "cfg.evaluation.metric = 'bbox' # Metrics used during evaluation\n",
    "\n",
    "# Set the epoch intervel to perform evaluation\n",
    "cfg.evaluation.interval = 1\n",
    "\n",
    "# Set the iou threshold of the mAP calculation during evaluation\n",
    "cfg.evaluation.iou_thrs = [0.5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "albu_train_transforms = [\n",
    "    dict(type='ShiftScaleRotate', shift_limit=0.0625,\n",
    "         scale_limit=0.15, rotate_limit=15, p=0.4),\n",
    "    dict(type='RandomBrightnessContrast', brightness_limit=0.2,\n",
    "         contrast_limit=0.2, p=0.5),\n",
    "    dict(type='IAAAffine', shear=(-10.0, 10.0), p=0.4),\n",
    "    dict(type=\"Blur\", p=1.0, blur_limit=7),\n",
    "    dict(type='CLAHE', p=0.5),\n",
    "    dict(type='Equalize', mode='cv', p=0.4),\n",
    "    dict(\n",
    "        type=\"OneOf\",\n",
    "        transforms=[\n",
    "            dict(type=\"GaussianBlur\", p=1.0, blur_limit=7),\n",
    "            dict(type=\"MedianBlur\", p=1.0, blur_limit=7),\n",
    "        ],\n",
    "        p=0.4,\n",
    "    ),\n",
    "    ]\n",
    "\n",
    "\n",
    "cfg.train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
    "    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
    "    dict(type='RandomFlip', flip_ratio=0.5),\n",
    "    dict(\n",
    "        type='Albu',\n",
    "        transforms=albu_train_transforms,\n",
    "        bbox_params=dict(\n",
    "        type='BboxParams',\n",
    "        format='pascal_voc',\n",
    "        label_fields=['gt_labels'],\n",
    "        min_visibility=0.0,\n",
    "        filter_lost_elements=True),\n",
    "        keymap=dict(img='image', gt_bboxes='bboxes'),\n",
    "        update_pad_shape=False,\n",
    "        skip_img_without_anno=True),\n",
    "    dict(\n",
    "        type='Normalize',\n",
    "        mean=[123.675, 116.28, 103.53],\n",
    "        std=[58.395, 57.12, 57.375],\n",
    "        to_rgb=True),\n",
    "    dict(type='Pad', size_divisor=32),\n",
    "    dict(type='DefaultFormatBundle'),\n",
    "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
    "]\n",
    "cfg.test_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(\n",
    "        type='MultiScaleFlipAug',\n",
    "        img_scale=(1333, 800),\n",
    "        flip=False,\n",
    "        transforms=[\n",
    "            dict(type='Resize', keep_ratio=True),\n",
    "            dict(type='RandomFlip'),\n",
    "            dict(\n",
    "                type='Normalize',\n",
    "                mean=[123.675, 116.28, 103.53],\n",
    "                std=[58.395, 57.12, 57.375],\n",
    "                to_rgb=True),\n",
    "            dict(type='Pad', size_divisor=32),\n",
    "            dict(type='ImageToTensor', keys=['img']),\n",
    "            dict(type='Collect', keys=['img'])\n",
    "        ])\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cfg.log_config.hooks = [dict(type='TextLoggerHook'),\n",
    "                        dict(type='WandbLoggerHook',\n",
    "                             init_kwargs=dict(project=wnb_project_name,\n",
    "                                              name=f'exp-{model_name}-fold{fold}-job{job}',\n",
    "                                              entity=wnb_username))\n",
    "                       ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cfg_path = f'{job_folder}/job{job}_{Path(baseline_cfg_path).name}'\n",
    "print(cfg_path)\n",
    "\n",
    "# Save config file for inference later\n",
    "cfg.dump(cfg_path)\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = build_detector(cfg.model,\n",
    "                       train_cfg=cfg.get('train_cfg'),\n",
    "                       test_cfg=cfg.get('test_cfg'))\n",
    "model.init_weights()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "datasets = [build_dataset(cfg.data.train)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_detector(model, datasets[0], cfg, distributed=False, validate=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the best epoch number\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "log_file = f'{job_folder}/None.log.json'\n",
    "\n",
    "# Source: mmdetection/tools/analysis_tools/analyze_logs.py \n",
    "def load_json_logs(json_logs):\n",
    "    # load and convert json_logs to log_dict, key is epoch, value is a sub dict\n",
    "    # keys of sub dict is different metrics, e.g. memory, bbox_mAP\n",
    "    # value of sub dict is a list of corresponding values of all iterations\n",
    "    log_dicts = [dict() for _ in json_logs]\n",
    "    for json_log, log_dict in zip(json_logs, log_dicts):\n",
    "        with open(json_log, 'r') as log_file:\n",
    "            for line in log_file:\n",
    "                log = json.loads(line.strip())\n",
    "                # skip lines without `epoch` field\n",
    "                if 'epoch' not in log:\n",
    "                    continue\n",
    "                epoch = log.pop('epoch')\n",
    "                if epoch not in log_dict:\n",
    "                    log_dict[epoch] = defaultdict(list)\n",
    "                for k, v in log.items():\n",
    "                    log_dict[epoch][k].append(v)\n",
    "    return log_dicts\n",
    "\n",
    "log_dict = load_json_logs([log_file])\n",
    "# [(print(inner['bbox_mAP']) for inner in item) for item in log_dict]\n",
    "# [print(item) for item in log_dict[0]]\n",
    "best_epoch = np.argmax([item['bbox_mAP'][0] for item in log_dict[0].values()])+1\n",
    "best_epoch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_files = [f'{job_folder}/epoch_{best_epoch}.pth',\n",
    "               cfg_path\n",
    "              ]\n",
    "\n",
    "# Create a new wnb run for saving models as artifacts\n",
    "run = wandb.init(project=wnb_project_name,\n",
    "                 name=f'models_files_{model_name}_fold{fold}_job{job}',\n",
    "                 entity=wnb_username,\n",
    "                 group='Artifact',\n",
    "                 job_type='model-files')\n",
    "\n",
    "artifact = wandb.Artifact(f'models_files_{model_name}_fold{fold}_job{job}', type='model')\n",
    "\n",
    "for model_file in model_files:\n",
    "    artifact.add_file(model_file)\n",
    "\n",
    "run.log_artifact(artifact)\n",
    "run.finish()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('yolo': conda)"
  },
  "interpreter": {
   "hash": "6228969df865c156c91fadb0937dc84af44d05a1f322ea4a1dd8ec555f33a733"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}